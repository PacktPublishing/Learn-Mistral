{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxan7E5LRgmB"
      },
      "source": [
        "## üìö Workshop: Parsing and Enriching Public Documents with NLP\n",
        "\n",
        "In this workshop, we will learn how to:\n",
        "\n",
        "1. Load real-world text content from a PDF document and a public HTML webpage.\n",
        "2. Clean and structure raw data using BeautifulSoup and pandas.\n",
        "3. Extract Named Entities using Hugging Face Transformers.\n",
        "4. Tag the content with relevant hashtags based on preloaded keyword dictionaries.\n",
        "\n",
        "Our use cases will include legal documents and natural science articles, preparing them for analysis or semantic indexing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih4qyXK8Rj5H"
      },
      "source": [
        "## üìÑ PDF Loader and Retriever\n",
        "\n",
        "We‚Äôll retrieve a U.S. copyright law PDF directly from a public website. Then we‚Äôll extract its text content using the `PyMuPDF` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEuscJsw8vI6"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iF_WbtQRnek"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import requests\n",
        "\n",
        "# Download a real public domain statute (Title 17)\n",
        "url = \"https://www.copyright.gov/title17/title17.pdf\"\n",
        "response = requests.get(url)\n",
        "with open(\"title17.pdf\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Extract text from pages\n",
        "doc = fitz.open(\"title17.pdf\")\n",
        "pdf_text = \"\\n\".join([page.get_text() for page in doc[:10]])  # limit to first 10 pages\n",
        "print(pdf_text[:1000])  # preview output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JER8zzXmRuRT"
      },
      "source": [
        "## üåê HTML Loader and Retriever\n",
        "\n",
        "Next, we‚Äôll fetch a public webpage (Wikipedia article on ducks) and extract its main paragraphs using `requests` and `BeautifulSoup`. CSS selectors help us locate only relevant sections like `<p>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqlgGxl7RyKD"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVc39pVIR2kw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Duck\"\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Select main paragraphs only\n",
        "paragraphs = soup.select(\"#mw-content-text .mw-parser-output > p\")\n",
        "wiki_text = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs if p.text.strip())[:5000]\n",
        "print(wiki_text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrnYPkVRR7oM"
      },
      "source": [
        "## üßπ BeautifulSoup + pandas Cleaning\n",
        "\n",
        "We‚Äôll now demonstrate basic cleaning and structuring with `pandas`, using the data from HTML or PDF as input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEnGzIZ3SBy_"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOlTI6OwSFhK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame([\n",
        "    {\"source\": \"title17.pdf\", \"text\": pdf_text},\n",
        "    {\"source\": \"wikipedia_duck\", \"text\": wiki_text}\n",
        "])\n",
        "\n",
        "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EprZ7OGrSIrz"
      },
      "source": [
        "## üß† Named Entity Recognition (NER)\n",
        "\n",
        "We‚Äôll use Hugging Face‚Äôs `bert-base-NER` model to extract names, locations, laws, species, and other entities from both documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb1wTPJvSQKQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbmM4MqESLvi"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
        "\n",
        "# Apply NER to each document (limit tokens)\n",
        "df[\"entities\"] = df[\"text\"].apply(lambda t: [e[\"word\"] for e in ner(t[:1000])][:10])\n",
        "df[[\"source\", \"entities\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay8W9wLwST6X"
      },
      "source": [
        "## üè∑ Hashtagging from Keywords\n",
        "\n",
        "We‚Äôll define two sets of domain-specific keywords for legal and birding content. These will be used to enrich documents with hashtags automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7MMNSdxSX9Z"
      },
      "outputs": [],
      "source": [
        "# Define keyword-to-hashtag maps\n",
        "legal_hashtags = {\n",
        "    \"copyright\": \"#copyrightlaw\",\n",
        "    \"intellectual property\": \"#IP\",\n",
        "    \"section 106\": \"#fairuse\",\n",
        "    \"DMCA\": \"#dmca\",\n",
        "    \"public domain\": \"#publicdomain\",\n",
        "    \"reproduction\": \"#reproductionrights\",\n",
        "    \"infringement\": \"#infringement\",\n",
        "    \"work of authorship\": \"#authorsrights\",\n",
        "    \"exclusive rights\": \"#exclusivity\",\n",
        "    \"derivative work\": \"#derivativework\"\n",
        "}\n",
        "\n",
        "birding_hashtags = {\n",
        "    \"duck\": \"#waterfowl\",\n",
        "    \"mallard\": \"#ducksofinstagram\",\n",
        "    \"wing\": \"#birdwatching\",\n",
        "    \"feathers\": \"#ornithology\",\n",
        "    \"migratory\": \"#birdmigration\",\n",
        "    \"quack\": \"#ducksounds\",\n",
        "    \"nest\": \"#birdnest\",\n",
        "    \"plumage\": \"#plumage\",\n",
        "    \"beak\": \"#birdanatomy\",\n",
        "    \"aquatic\": \"#wetlandbirds\"\n",
        "}\n",
        "\n",
        "# Map hashtags based on content\n",
        "def tag_text(text, tag_dict):\n",
        "    return [tag for key, tag in tag_dict.items() if key.lower() in text.lower()][:10]\n",
        "\n",
        "df[\"hashtags\"] = df.apply(\n",
        "    lambda row: tag_text(row[\"text\"], legal_hashtags if \"title17\" in row[\"source\"] else birding_hashtags),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df[[\"source\", \"hashtags\"]]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
